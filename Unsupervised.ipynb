{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## requirement"
      ],
      "metadata": {
        "id": "9uyscYwuG0ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py_vncorenlp gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-tTsdH66Isa",
        "outputId": "226cb57f-ed91-49e0-cb72-ad996137df4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py_vncorenlp in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: pyjnius in /usr/local/lib/python3.10/dist-packages (from py_vncorenlp) (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "3c8KW2WcG3c7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbD5KQF_GKgQ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import math\n",
        "import py_vncorenlp\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "YBvYkbmcG5nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train\n",
        "!cd train && gdown https://drive.google.com/uc?id=1nzak5OkrheRV1ltOGCXkT671bmjODLhP&export=download\n",
        "!cd train && gdown https://drive.google.com/uc?id=1ye-gOZIBqXdKOoi_YxvpT6FeRNmViPPv&export=download\n",
        "!mkdir test\n",
        "!cd test && gdown https://drive.google.com/uc?id=1aNMOeZZbNwSRkjyCWAGtNCMa3YrshR-n&export=download\n",
        "!cd test && gdown https://drive.google.com/uc?id=1vkQS5gI0is4ACU58-AbWusnemw7KZNfO&export=download"
      ],
      "metadata": {
        "id": "-bljX-QzG7SE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c35f48-35ee-49a6-99f6-45fe0e2a2bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘train’: File exists\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nzak5OkrheRV1ltOGCXkT671bmjODLhP\n",
            "To: /content/train/sents.txt\n",
            "100% 898k/898k [00:00<00:00, 113MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ye-gOZIBqXdKOoi_YxvpT6FeRNmViPPv\n",
            "To: /content/train/sentiments.txt\n",
            "100% 22.9k/22.9k [00:00<00:00, 35.8MB/s]\n",
            "mkdir: cannot create directory ‘test’: File exists\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aNMOeZZbNwSRkjyCWAGtNCMa3YrshR-n\n",
            "To: /content/test/sents.txt\n",
            "100% 248k/248k [00:00<00:00, 80.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vkQS5gI0is4ACU58-AbWusnemw7KZNfO\n",
            "To: /content/test/sentiments.txt\n",
            "100% 6.33k/6.33k [00:00<00:00, 10.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train/sents.txt') as data, open('train/sentiments.txt') as label:\n",
        "  traindata = []\n",
        "  for dataline, labelline in zip(data, label):\n",
        "    sentence = dataline.strip()\n",
        "    sentiment = int(labelline.strip())\n",
        "    if sentiment == 1:\n",
        "      continue\n",
        "    traindata.append((sentence, sentiment))\n",
        "print(len(traindata))\n",
        "traindata[12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7UsyxeF_kw1",
        "outputId": "d474d18c-8253-4715-9d66-9df0d327d0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11426\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('đang dạy thầy wzjwz208 đi qua nước ngoài giữa chừng , thầy wzjwz209 dạy thay .',\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test/sents.txt') as data, open('test/sentiments.txt') as label:\n",
        "  testdata = []\n",
        "  for dataline, labelline in zip(data, label):\n",
        "    sentence = dataline.strip()\n",
        "    sentiment = int(labelline.strip())\n",
        "    if sentiment == 1:\n",
        "      continue\n",
        "    testdata.append((sentence, sentiment))\n",
        "print(len(testdata))\n",
        "testdata[79]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M70mhK8p6zHM",
        "outputId": "b7428c55-4bfe-4984-a0de-0b66c882c24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3166\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('giảng bài xúc tích .', 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## phrase"
      ],
      "metadata": {
        "id": "fuOn7tEcLr62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "py_vncorenlp.download_model(save_dir='./')"
      ],
      "metadata": {
        "id": "zL05t3xVG-MM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ac7197-e2a0-45d1-eaee-902353b3635d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VnCoreNLP model folder . already exists! Please load VnCoreNLP from this folder!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phraseModel = py_vncorenlp.VnCoreNLP(annotators=['wseg', 'pos'], save_dir='./')"
      ],
      "metadata": {
        "id": "5E48E1uM6ZVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPos(sentence):\n",
        "  annotated = phraseModel.annotate_text(sentence)\n",
        "  words = [word['wordForm'] for word in annotated[0]]\n",
        "  tags = [word['posTag'] for word in annotated[0]]\n",
        "  return words, tags"
      ],
      "metadata": {
        "id": "wvhuVB_a77NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words, tags = getPos(traindata[999][0])\n",
        "print(words)\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uekbfbPJ8cJt",
        "outputId": "f09914c8-965d-43b7-900c-5ad1d006db0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cô', 'dạy', 'có', 'tâm', ',', 'post', 'tài_liệu', 'và', 'bài_tập', 'lên', 'moodle', 'rất', 'đầy_đủ', '.']\n",
            "['N', 'V', 'V', 'N', 'CH', 'N', 'N', 'Cc', 'N', 'V', 'N', 'R', 'A', 'CH']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getPhrase(words, tags):\n",
        "  phrases = []\n",
        "  ptags = []\n",
        "  for idx, word in enumerate(words):\n",
        "    phrase = tuple(words[idx:idx + 2])\n",
        "    ptag = tuple(tags[idx:idx + 2])\n",
        "    if ptag in [('N', 'A'), ('V', 'A'), ('R', 'A'), ('R', 'V'), ('V', 'R')]:\n",
        "      phrases.append(phrase)\n",
        "      ptags.append(ptag)\n",
        "  return phrases, ptags"
      ],
      "metadata": {
        "id": "16fdSX8G8tlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrases, ptags = getPhrase(words, tags)\n",
        "print(phrases)\n",
        "print(ptags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiEPiq8q9pa-",
        "outputId": "1398590a-9149-4aba-b11d-1d79ba9c32ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('rất', 'đầy_đủ')]\n",
            "[('R', 'A')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pmi"
      ],
      "metadata": {
        "id": "qLVaadEcG8dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PMIModel:\n",
        "  def __init__(self, traindata):\n",
        "    self.map1 = {}\n",
        "    self.map2 = {}\n",
        "    posCount = 0\n",
        "    globalCount = 0\n",
        "    for sentence, label in tqdm(traindata):\n",
        "      if label == 2:\n",
        "        posCount += 1\n",
        "      words, tags = getPos(sentence)\n",
        "      phrases, ptags = getPhrase(words, tags)\n",
        "      for p in phrases:\n",
        "        globalCount += 1\n",
        "        self.map1[p] = self.map1.get(p, 0) + 1\n",
        "        p2 = (p, label)\n",
        "        self.map2[p2] = self.map2.get(p2, 0) + 1\n",
        "    print(len(self.map1))\n",
        "    for key, val in self.map1.items():\n",
        "      self.map1[key] = val / globalCount\n",
        "    for key, val in self.map2.items():\n",
        "      self.map2[key] = val / globalCount\n",
        "    self.map1[0] = 1.0 - posCount / len(traindata)\n",
        "    self.map1[2] = posCount / len(traindata)\n",
        "\n",
        "  def getProb(self, p):\n",
        "    return self.map1.get(p, 0.0) + 0.01\n",
        "\n",
        "  def getProb2(self, p1, p2):\n",
        "    return self.map2.get((p1, p2), 0.0) + 0.01\n",
        "\n",
        "  def predict(self, sentence):\n",
        "    words, tags = getPos(sentence)\n",
        "    phrases, ptags = getPhrase(words, tags)\n",
        "    so = 0.0\n",
        "    for p in phrases:\n",
        "      sop2 = self.getProb2(p, 2) / (self.getProb(p) * self.getProb(2))\n",
        "      sop0 = self.getProb2(p, 0) / (self.getProb(p) * self.getProb(0))\n",
        "      so += math.log2(sop2) - math.log2(sop0)\n",
        "    return 2 if so >= 0 else 0\n",
        "\n",
        "  def test(self, dataset):\n",
        "    hitCount = 0\n",
        "    yTrue = []\n",
        "    yPred = []\n",
        "    for sentence, label in tqdm(dataset):\n",
        "      predict = self.predict(sentence)\n",
        "      yTrue.append(label)\n",
        "      yPred.append(predict)\n",
        "      if predict == label:\n",
        "        hitCount += 1\n",
        "    print(f'{hitCount}/{len(dataset)} ~{hitCount / len(dataset) * 100}')\n",
        "    f1Score = f1_score(yTrue, yPred, average='weighted')\n",
        "    print(f'f1 score: {f1Score}')"
      ],
      "metadata": {
        "id": "ABw9uaU2Eh3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmiModel = PMIModel(traindata)\n",
        "pmiModel.test(testdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtZSAsIxIOUz",
        "outputId": "06b10f67-9bbf-4695-eea6-49787a5b943c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10968/10968 [00:11<00:00, 984.06it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2999/2999 [00:02<00:00, 1202.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2163/2999 ~72.1240413471157\n",
            "f1 score: 0.7176818017124736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word2vec"
      ],
      "metadata": {
        "id": "Ms08OSwNG-vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import torch\n",
        "import torch.nn.functional as torchF\n",
        "\n",
        "class W2VModel:\n",
        "  def __init__(self, traindata):\n",
        "    self.sentences = []\n",
        "    for sentence, label in tqdm(traindata):\n",
        "      words, tags = getPos(sentence)\n",
        "      self.sentences.append(words)\n",
        "    self.vectorSize = 100\n",
        "    self.w2v = Word2Vec(sentences=self.sentences, vector_size=self.vectorSize, window=5, min_count=1, workers=2)\n",
        "    print()\n",
        "    print(self.w2v.wv.vectors.shape)\n",
        "\n",
        "  def embed(self, word):\n",
        "    if word in self.w2v.wv.key_to_index:\n",
        "      return torch.Tensor(self.w2v.wv[word])\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def embed2(self, phrase):\n",
        "    res = torch.zeros(self.vectorSize)\n",
        "    resLen = 0\n",
        "    for word in phrase:\n",
        "      wres = self.embed(word)\n",
        "      if wres is None:\n",
        "        continue\n",
        "      res += wres\n",
        "      resLen += 1\n",
        "    return res / resLen\n",
        "\n",
        "  def predict(self, sentence):\n",
        "    posEp = self.embed('tốt')\n",
        "    negEp = self.embed('kém')\n",
        "    words, tags = getPos(sentence)\n",
        "    phrases, ptags = getPhrase(words, tags)\n",
        "    so = 0.0\n",
        "    for p in phrases:\n",
        "      ep = self.embed2(p)\n",
        "      posSim = torchF.cosine_similarity(ep, posEp, dim=0)\n",
        "      negSim = torchF.cosine_similarity(ep, negEp, dim=0)\n",
        "      so += posSim - negSim\n",
        "    return 2 if so >= 0 else 0\n",
        "\n",
        "  def test(self, dataset):\n",
        "    hitCount = 0\n",
        "    yTrue = []\n",
        "    yPred = []\n",
        "    for sentence, label in tqdm(dataset):\n",
        "      predict = self.predict(sentence)\n",
        "      yTrue.append(label)\n",
        "      yPred.append(predict)\n",
        "      if predict == label:\n",
        "        hitCount += 1\n",
        "    print(f'{hitCount}/{len(dataset)} ~{hitCount / len(dataset) * 100}')\n",
        "    f1Score = f1_score(yTrue, yPred, average='weighted')\n",
        "    print(f'f1 score: {f1Score}')"
      ],
      "metadata": {
        "id": "RWqpAP1uHAGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2vModel = W2VModel(traindata)\n",
        "w2vModel.test(testdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FJiHbPbtGEZ",
        "outputId": "da1294ea-dabc-41ff-ccb2-3c7e580f0362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10968/10968 [00:09<00:00, 1147.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(3568, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2999 [00:00<?, ?it/s]<ipython-input-33-d97114ddf32d>:18: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  return torch.Tensor(self.w2v.wv[word])\n",
            "100%|██████████| 2999/2999 [00:03<00:00, 785.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2284/2999 ~76.15871957319106\n",
            "f1 score: 0.76117136121854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with neutral"
      ],
      "metadata": {
        "id": "EKSzqoQg_nv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train/sents.txt') as data, open('train/sentiments.txt') as label:\n",
        "  traindata = []\n",
        "  for dataline, labelline in zip(data, label):\n",
        "    sentence = dataline.strip()\n",
        "    sentiment = int(labelline.strip())\n",
        "    traindata.append((sentence, sentiment))\n",
        "print(len(traindata))\n",
        "traindata[12]"
      ],
      "metadata": {
        "id": "IAhd363iEpMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test/sents.txt') as data, open('test/sentiments.txt') as label:\n",
        "  testdata = []\n",
        "  for dataline, labelline in zip(data, label):\n",
        "    sentence = dataline.strip()\n",
        "    sentiment = int(labelline.strip())\n",
        "    testdata.append((sentence, sentiment))\n",
        "print(len(testdata))\n",
        "testdata[79]"
      ],
      "metadata": {
        "id": "0gu0waWBErH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PMIModel:\n",
        "  def __init__(self, traindata):\n",
        "    self.map1 = {}\n",
        "    self.map2 = {}\n",
        "    posCount = 0\n",
        "    globalCount = 0\n",
        "    for sentence, label in tqdm(traindata):\n",
        "      if label == 2:\n",
        "        posCount += 1\n",
        "      words, tags = getPos(sentence)\n",
        "      phrases, ptags = getPhrase(words, tags)\n",
        "      for p in phrases:\n",
        "        globalCount += 1\n",
        "        self.map1[p] = self.map1.get(p, 0) + 1\n",
        "        p2 = (p, label)\n",
        "        self.map2[p2] = self.map2.get(p2, 0) + 1\n",
        "    print(len(self.map1))\n",
        "    for key, val in self.map1.items():\n",
        "      self.map1[key] = val / globalCount\n",
        "    for key, val in self.map2.items():\n",
        "      self.map2[key] = val / globalCount\n",
        "    self.map1[0] = 1.0 - posCount / len(traindata)\n",
        "    self.map1[2] = posCount / len(traindata)\n",
        "\n",
        "  def getProb(self, p):\n",
        "    return self.map1.get(p, 0.0) + 0.01\n",
        "\n",
        "  def getProb2(self, p1, p2):\n",
        "    return self.map2.get((p1, p2), 0.0) + 0.01\n",
        "\n",
        "  def predict(self, sentence):\n",
        "    words, tags = getPos(sentence)\n",
        "    phrases, ptags = getPhrase(words, tags)\n",
        "    so = 0.0\n",
        "    for p in phrases:\n",
        "      sop2 = self.getProb2(p, 2) / (self.getProb(p) * self.getProb(2))\n",
        "      sop0 = self.getProb2(p, 0) / (self.getProb(p) * self.getProb(0))\n",
        "      so += math.log2(sop2) - math.log2(sop0)\n",
        "    if abs(so) <= 1e-8:\n",
        "      return 1\n",
        "    return 2 if so >= 0 else 0\n",
        "\n",
        "  def test(self, dataset):\n",
        "    hitCount = 0\n",
        "    yTrue = []\n",
        "    yPred = []\n",
        "    for sentence, label in tqdm(dataset):\n",
        "      predict = self.predict(sentence)\n",
        "      yTrue.append(label)\n",
        "      yPred.append(predict)\n",
        "      if predict == label:\n",
        "        hitCount += 1\n",
        "    print(f'{hitCount}/{len(dataset)} ~{hitCount / len(dataset) * 100}')\n",
        "    f1Score = f1_score(yTrue, yPred, average='weighted')\n",
        "    print(f'f1 score: {f1Score}')"
      ],
      "metadata": {
        "id": "vLgUerFq8rW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmiModel = PMIModel(traindata)\n",
        "pmiModel.test(testdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Po1_dnh_qRL",
        "outputId": "33a4eb8e-28b3-4236-ffe0-cf248c393e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11426/11426 [00:11<00:00, 978.03it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3166/3166 [00:02<00:00, 1087.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1952/3166 ~61.65508528111181\n",
            "f1 score: 0.6249543992345439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import torch\n",
        "import torch.nn.functional as torchF\n",
        "\n",
        "class W2VModel:\n",
        "  def __init__(self, traindata):\n",
        "    self.sentences = []\n",
        "    for sentence, label in tqdm(traindata):\n",
        "      words, tags = getPos(sentence)\n",
        "      self.sentences.append(words)\n",
        "    self.vectorSize = 100\n",
        "    self.w2v = Word2Vec(sentences=self.sentences, vector_size=self.vectorSize, window=5, min_count=1, workers=2)\n",
        "    print()\n",
        "    print(self.w2v.wv.vectors.shape)\n",
        "\n",
        "  def embed(self, word):\n",
        "    if word in self.w2v.wv.key_to_index:\n",
        "      return torch.Tensor(self.w2v.wv[word])\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def embed2(self, phrase):\n",
        "    res = torch.zeros(self.vectorSize)\n",
        "    resLen = 0\n",
        "    for word in phrase:\n",
        "      wres = self.embed(word)\n",
        "      if wres is None:\n",
        "        continue\n",
        "      res += wres\n",
        "      resLen += 1\n",
        "    return res / resLen\n",
        "\n",
        "  def predict(self, sentence):\n",
        "    posEp = self.embed('tốt')\n",
        "    negEp = self.embed('kém')\n",
        "    words, tags = getPos(sentence)\n",
        "    phrases, ptags = getPhrase(words, tags)\n",
        "    so = 0.0\n",
        "    for p in phrases:\n",
        "      ep = self.embed2(p)\n",
        "      posSim = torchF.cosine_similarity(ep, posEp, dim=0)\n",
        "      negSim = torchF.cosine_similarity(ep, negEp, dim=0)\n",
        "      so += posSim - negSim\n",
        "    if abs(so) <= 1e-8:\n",
        "      return 1\n",
        "    return 2 if so >= 0 else 0\n",
        "\n",
        "  def test(self, dataset):\n",
        "    hitCount = 0\n",
        "    yTrue = []\n",
        "    yPred = []\n",
        "    for sentence, label in tqdm(dataset):\n",
        "      predict = self.predict(sentence)\n",
        "      yTrue.append(label)\n",
        "      yPred.append(predict)\n",
        "      if predict == label:\n",
        "        hitCount += 1\n",
        "    print(f'{hitCount}/{len(dataset)} ~{hitCount / len(dataset) * 100}')\n",
        "    f1Score = f1_score(yTrue, yPred, average='weighted')\n",
        "    print(f'f1 score: {f1Score}')"
      ],
      "metadata": {
        "id": "HlAbnDF_Aebk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2vModel = W2VModel(traindata)\n",
        "w2vModel.test(testdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euFJxZMtBN6v",
        "outputId": "62a17c50-0f08-4c68-b6be-cae5d7cd2d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11426/11426 [00:11<00:00, 955.88it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(3655, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3166/3166 [00:05<00:00, 613.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2133/3166 ~67.37207833228048\n",
            "f1 score: 0.6968596778073534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOJJEa7YCGQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}